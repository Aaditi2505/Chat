# -*- coding: utf-8 -*-
"""add2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17H7GeXLk5E9wbHMdY1tS4bPDVAQNMKMj
"""
 



import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from gtts import gTTS
import os
import uuid
import gradio as gr
import whisper
import shutil # Import shutil for directory cleanup

# --- Configuration ---
# Define the path for saving temporary audio files
AUDIO_OUTPUT_DIR = "temp_audio"
# Ensure the directory exists; create it if it doesn't
os.makedirs(AUDIO_OUTPUT_DIR, exist_ok=True)

# --- Data Loading and Model Training ---
try:
    # Load FAQ data from the CSV file
    df = pd.read_csv("kamaraj_college_faq.csv")
    # Drop rows with any missing values to ensure clean data for training
    df.dropna(inplace=True)
    print("FAQ data loaded successfully.")
except FileNotFoundError:
    # If the CSV file is not found, print an error and exit
    print("Error: 'kamaraj_college_faq.csv' not found. Please ensure the FAQ data file is in the same directory.")
    print("Exiting program.")
    exit()

# Encode answers into numerical labels for the model
label_encoder = LabelEncoder()
df["Answer_Label"] = label_encoder.fit_transform(df["Answer"])
print("Answers encoded.")

# Vectorize questions using TF-IDF to convert text into numerical features
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df["Question"]) # Features (questions)
y = df["Answer_Label"] # Target (encoded answers)
print("Questions vectorized.")

# Train a Logistic Regression model
# Increased max_iter to help the model converge for complex datasets
model = LogisticRegression(max_iter=1000)
model.fit(X, y)
print("Model trained successfully.")

# --- Text-to-Speech Function ---
def save_tts_audio(text):
    """
    Generates an audio file from text using Google Text-to-Speech (gTTS)
    and saves it to the temporary audio directory.
    Returns the file path of the saved audio.
    """
    tts = gTTS(text=text, lang='en')
    # Create a unique filename to prevent conflicts
    filename = os.path.join(AUDIO_OUTPUT_DIR, f"response_{uuid.uuid4().hex}.mp3")
    tts.save(filename)
    return filename

# --- Load Whisper Model ---
# Load the Whisper ASR model for transcribing audio.
# The "base" model is a good balance of size and accuracy for general use.
try:
    print("Loading Whisper model (this may take a moment on first run)...")
    whisper_model = whisper.load_model("base")
    print("Whisper model loaded successfully.")
except Exception as e:
    print(f"Error loading Whisper model: {e}")
    print("Please ensure you have the 'whisper' library installed and an internet connection for the first-time download.")
    print("Exiting program.")
    exit()

# --- Chatbot Logic ---
def chatbot(audio=None, text=None):
    """
    The core chatbot function that handles user input (audio or text).
    It transcribes audio if provided, predicts an answer, generates speech for the answer,
    and returns both the text response and the path to the audio file.
    """
    user_input = ""
    response_audio_path = None # Will store the path to the spoken answer

    # Process audio input if available
    if audio:
        try:
            print(f"Transcribing audio from: {audio}")
            result = whisper_model.transcribe(audio)
            user_input = result["text"]
            print(f"Audio transcribed: '{user_input}'")
        except Exception as e:
            print(f"Error during audio transcription: {e}")
            return "‚ùó An error occurred during audio transcription. Please try again or type your question.", None
    # Process text input if audio is not available
    elif text:
        user_input = text
        print(f"Received text input: '{user_input}'")
    else:
        # If no input is provided, prompt the user
        return "‚ùó Please ask a question via text or voice.", None

    # Validate that the user input is not empty after processing
    if not user_input.strip():
        return "‚ùó It seems you didn't provide a clear question. Please try again.", None

    # Use the trained model to predict the answer
    vec = vectorizer.transform([user_input]) # Transform user input using the trained vectorizer
    prediction = model.predict(vec)[0] # Get the predicted label
    answer = label_encoder.inverse_transform([prediction])[0] # Convert label back to original answer text
    print(f"Predicted answer: '{answer}'")

    # Generate and save the audio for the predicted answer
    response_audio_path = save_tts_audio(answer)
    print(f"Answer audio saved to: {response_audio_path}")

    # Return the text response and the audio file path for Gradio to display
    # Using Markdown for the text output to allow for bold formatting
    return f"üó£Ô∏è You asked: **{user_input}**\n‚úÖ Answer: **{answer}**", response_audio_path

# --- Gradio Interface ---
print("Setting up Gradio interface...")
iface = gr.Interface(
    fn=chatbot, # The Python function to call when the UI is interacted with
    inputs=[
        # Input for microphone audio
        gr.Audio(sources=["microphone"], type="filepath", label="üé§ Speak your question"),
        # Input for typing questions
        gr.Textbox(lines=2, placeholder="Or type your question here", label="üìù Type your question")
    ],
    outputs=[
        # Output for the text response (using Markdown for rich text)
        gr.Markdown(label="Chatbot Response"),
        # Output for the spoken answer (Gradio will render an audio player)
        gr.Audio(label="Spoken Answer", type="filepath", autoplay=True) # autoplay=True plays audio automatically
    ],
    title="üéì Kamaraj College FAQ Chatbot",
    description="Ask about Kamaraj College by voice or text. The chatbot will respond and speak the answer.",
    allow_flagging="never", # Disables the "Flag" button for feedback (often not needed for demos)
    theme="soft" # Applies a pleasant, soft visual theme to the Gradio UI
)

print("Launching Gradio interface. Open the public URL in your browser.")
# Launch the Gradio interface. It will provide a local URL and potentially a public share link.
iface.launch()

# --- Cleanup ---
# This section ensures that the temporary audio files are deleted when the script exits.
def cleanup_audio_files():
    if os.path.exists(AUDIO_OUTPUT_DIR):
        print(f"Cleaning up temporary audio directory: {AUDIO_OUTPUT_DIR}")
        # Remove the directory and all its contents
        shutil.rmtree(AUDIO_OUTPUT_DIR)

import atexit
# Register the cleanup function to be called automatically when the program terminates
atexit.register(cleanup_audio_files)
